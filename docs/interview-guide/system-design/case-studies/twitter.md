---
sidebar_position: 2
title: "Design Twitter â€” Social Network at Scale"
description: >-
  Complete system design for Twitter. Timeline generation, fan-out strategies,
  celebrity problem, and real-time notifications.
keywords:
  - design twitter
  - social network design
  - timeline
  - fan-out
  - news feed
  - system design
difficulty: Advanced
estimated_time: 60 minutes
prerequisites:
  - URL Shortener Case Study
  - Caching
  - Message Queues
  - SQL vs NoSQL
companies: [Twitter, Meta, Google, Amazon, Microsoft]
---

# Design Twitter: The Timeline Challenge

Twitter seems simple: 280 characters, click tweet. But beneath that simplicity lies one of the most challenging distributed systems problems.

**The core challenge:** When Lady Gaga (80M followers) tweets, how do 80 million timelines update instantly?

---

## Phase 1: Requirements (5 minutes)

### Functional Requirements

| Feature | Priority |
|---------|----------|
| Post tweets (280 chars, images) | Must have |
| Follow/unfollow users | Must have |
| Home timeline (tweets from followed users) | Must have |
| User timeline (user's own tweets) | Must have |
| Search tweets | Nice to have |
| Notifications | Nice to have |
| Trending topics | Nice to have |

### Non-Functional Requirements

| Aspect | Requirement |
|--------|-------------|
| **Scale** | 500M users, 200M DAU |
| **Timeline latency** | < 200ms |
| **Availability** | 99.99% |
| **Consistency** | Eventual (seconds) |

### Clarifying Questions

- "Real-time or near-real-time?" â†’ Near-real-time (seconds OK)
- "Media support?" â†’ Images and videos
- "Celebrity users special handling?" â†’ Yes, discuss later

---

## Phase 2: Capacity Estimation (5 minutes)

### Traffic

```
Users: 500M total, 200M DAU
Average follows: 200 users
Average followers: 200 users

Tweets per day:
- 200M DAU Ã— 2 tweets/day = 400M tweets/day
- 400M / 86,400 = ~4,600 tweets/second
- Peak (2x): ~10,000 tweets/second

Timeline reads:
- 200M DAU Ã— 10 views/day = 2B reads/day
- 2B / 86,400 = ~23,000 reads/second
- Peak: ~50,000 reads/second

Read:Write ratio = 50,000:10,000 = 5:1
```

### Storage

```
Tweet storage:
- Tweet text: 280 bytes
- Metadata: 100 bytes
- Total: ~400 bytes/tweet

Daily: 400M Ã— 400 bytes = 160 GB/day
Yearly: 160 GB Ã— 365 = ~60 TB/year

Media storage:
- 10% of tweets have images
- Average image: 1 MB
- 40M images Ã— 1 MB = 40 TB/day
```

---

## Phase 3: High-Level Design (15 minutes)

### API Design

```
POST /tweets
Request: {
  "user_id": "123",
  "content": "Hello world!",
  "media_ids": ["img_1"]
}
Response: { "tweet_id": "abc123", "created_at": "..." }

GET /timeline/home/{user_id}
Response: {
  "tweets": [...],
  "cursor": "next_page_token"
}

GET /timeline/user/{user_id}
Response: { "tweets": [...] }

POST /follow
Request: { "follower_id": "123", "followee_id": "456" }

GET /search?q=keyword
Response: { "tweets": [...] }
```

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Clients â”‚â”€â”€â”€â”€â–¶â”‚ Load Balancer  â”‚â”€â”€â”€â”€â–¶â”‚   API Gateway    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                      â”‚                                      â”‚
         â–¼                                      â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tweet Service  â”‚                   â”‚Timeline Service â”‚                    â”‚  User Service   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                     â”‚                                      â”‚
         â–¼                                     â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Tweet DB      â”‚                   â”‚  Timeline Cache â”‚                    â”‚    User DB      â”‚
â”‚  (Cassandra)    â”‚                   â”‚    (Redis)      â”‚                    â”‚  (PostgreSQL)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Message Queue  â”‚â”€â”€â”€â”€â”€â”€â–¶ Fan-out Service
â”‚    (Kafka)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 4: Deep Dive (15 minutes)

### The Timeline Problem

**How do you generate a user's home timeline?**

Two approaches:

### Approach 1: Fan-out on Write (Push)

When user tweets, push to all followers' timelines.

```
User A tweets
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Fan-out Service                       â”‚
â”‚                                                 â”‚
â”‚   Get followers of A: [B, C, D, ...]           â”‚
â”‚   For each follower:                           â”‚
â”‚       Prepend tweet to their timeline cache    â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â–¶ Redis: timeline:B â†’ [new_tweet, ...]
    â”œâ”€â”€â–¶ Redis: timeline:C â†’ [new_tweet, ...]
    â””â”€â”€â–¶ Redis: timeline:D â†’ [new_tweet, ...]
```

**Pros:**
- Timeline reads are fast (pre-computed)
- Simple read path

**Cons:**
- Celebrity problem: 80M writes per tweet
- Wasted work for inactive users
- Slower write path

### Approach 2: Fan-out on Read (Pull)

When user requests timeline, fetch from followed users.

```
User B requests timeline
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Timeline Service                      â”‚
â”‚                                                 â”‚
â”‚   Get users B follows: [A, X, Y, ...]          â”‚
â”‚   For each followed user:                      â”‚
â”‚       Fetch recent tweets                      â”‚
â”‚   Merge and sort by time                       â”‚
â”‚   Return top N                                 â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- No celebrity problem
- No wasted work

**Cons:**
- Slow reads (N queries + merge)
- High read latency

### Hybrid Approach (Twitter's Solution)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hybrid Fan-out                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Regular users (< 10K followers):                           â”‚
â”‚  â””â”€â”€ Fan-out on write (push to followers)                   â”‚
â”‚                                                             â”‚
â”‚  Celebrities (> 10K followers):                             â”‚
â”‚  â””â”€â”€ Don't fan-out                                          â”‚
â”‚  â””â”€â”€ Merge at read time                                     â”‚
â”‚                                                             â”‚
â”‚  Timeline read:                                             â”‚
â”‚  1. Fetch pre-computed timeline (Redis)                     â”‚
â”‚  2. Fetch recent tweets from followed celebrities           â”‚
â”‚  3. Merge and return                                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Models

**Tweet Storage (Cassandra)**

```sql
-- Tweets table (write-optimized)
CREATE TABLE tweets (
    tweet_id UUID,
    user_id UUID,
    content TEXT,
    media_urls LIST<TEXT>,
    created_at TIMESTAMP,
    PRIMARY KEY (tweet_id)
);

-- User timeline (for user's own tweets)
CREATE TABLE user_timeline (
    user_id UUID,
    tweet_id UUID,
    created_at TIMESTAMP,
    PRIMARY KEY (user_id, created_at)
) WITH CLUSTERING ORDER BY (created_at DESC);
```

**Timeline Cache (Redis)**

```python
# Home timeline: sorted set by timestamp
ZADD timeline:{user_id} {timestamp} {tweet_id}

# Limit to last 800 tweets
ZREMRANGEBYRANK timeline:{user_id} 0 -801

# Fetch timeline
ZREVRANGE timeline:{user_id} 0 100
```

**Social Graph (PostgreSQL or Graph DB)**

```sql
CREATE TABLE follows (
    follower_id BIGINT,
    followee_id BIGINT,
    created_at TIMESTAMP,
    PRIMARY KEY (follower_id, followee_id)
);

CREATE INDEX idx_followee ON follows(followee_id);
```

### Tweet Flow

```
1. User creates tweet
   â”‚
   â–¼
2. Tweet Service
   â”œâ”€â”€ Validate content
   â”œâ”€â”€ Store in Tweet DB
   â””â”€â”€ Publish to Kafka
         â”‚
         â–¼
3. Fan-out Service (async)
   â”œâ”€â”€ Check if user is celebrity (>10K followers)
   â”‚   â”œâ”€â”€ Yes: Skip fan-out
   â”‚   â””â”€â”€ No: Continue
   â”‚
   â”œâ”€â”€ Get follower list
   â””â”€â”€ For each active follower:
       â””â”€â”€ ZADD to Redis timeline
         â”‚
         â–¼
4. Notification Service (async)
   â””â”€â”€ Push notifications to mentioned users
```

### Timeline Read Flow

```
1. User requests home timeline
   â”‚
   â–¼
2. Timeline Service
   â”œâ”€â”€ Fetch pre-computed timeline from Redis
   â”‚   â””â”€â”€ ZREVRANGE timeline:{user_id} 0 100
   â”‚
   â”œâ”€â”€ Get list of celebrities user follows
   â”‚
   â”œâ”€â”€ Fetch recent tweets from each celebrity
   â”‚   â””â”€â”€ Query Tweet DB or cache
   â”‚
   â”œâ”€â”€ Merge all tweets by timestamp
   â”‚
   â””â”€â”€ Return top N tweets with pagination
```

### Search

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Search Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Tweet Created                                              â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  Kafka â”€â”€â–¶ Search Indexer â”€â”€â–¶ Elasticsearch                â”‚
â”‚                                                             â”‚
â”‚  Search Query                                               â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  API â”€â”€â–¶ Elasticsearch â”€â”€â–¶ Tweet IDs â”€â”€â–¶ Tweet DB          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 5: Scaling & Reliability

### Scaling Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Global Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚                        DNS                                  â”‚
â”‚                         â”‚                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â”‚               â”‚               â”‚                  â”‚
â”‚         â–¼               â–¼               â–¼                  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚    â”‚  US-West â”‚    â”‚ US-East â”‚    â”‚  Europe  â”‚             â”‚
â”‚    â”‚  Region  â”‚    â”‚  Region â”‚    â”‚  Region  â”‚             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚               â”‚               â”‚                  â”‚
â”‚    Cassandra       Cassandra       Cassandra              â”‚
â”‚    (replicated)    (replicated)    (replicated)           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Database Sharding

```
Tweet DB: Shard by tweet_id (consistent hashing)
Timeline Cache: Shard by user_id
Social Graph: Shard by user_id
```

### Handling Failures

| Component | Failure Strategy |
|-----------|-----------------|
| Tweet Service | Retry queue, idempotency |
| Timeline Cache | Fallback to DB, rebuild |
| Fan-out Service | Kafka replay, at-least-once |
| Search | Graceful degradation |

---

## Summary

### Key Decisions

| Decision | Choice | Reasoning |
|----------|--------|-----------|
| Timeline strategy | Hybrid fan-out | Balance celebrity problem |
| Tweet storage | Cassandra | Write throughput |
| Timeline cache | Redis | Fast reads, sorted sets |
| Social graph | PostgreSQL | Relationship queries |
| Async processing | Kafka | Decouple, replay |

### Trade-offs

| Trade-off | Our Choice |
|-----------|------------|
| Consistency vs Latency | Eventual consistency (seconds) |
| Storage vs Compute | Pre-compute timelines (more storage) |
| Complexity vs Performance | Hybrid approach (more complex) |

---

## Key Takeaways

1. **Fan-out on write for regular users**, fan-out on read for celebrities.

2. **Timeline cache in Redis** with sorted sets for O(log N) inserts.

3. **Cassandra for tweets** â€” write-optimized, time-series friendly.

4. **Async processing via Kafka** for fan-out and notifications.

5. **The celebrity problem** is the key challenge to discuss.

---

## What's Next?

Design a chat system with real-time messaging:

ğŸ‘‰ [Design WhatsApp â†’](./whatsapp)
